---
source: src/main.rs
expression: scanned
input_file: test-data/lua5.4-tests/cstack.lua
---
[
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 5,
    },
    Token {
        kind: Identifier,
        lexeme: "tracegc",
        computed_lexeme: None,
        line: 5,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 5,
    },
    Token {
        kind: Identifier,
        lexeme: "require",
        computed_lexeme: None,
        line: 5,
    },
    Token {
        kind: String,
        lexeme: "\"tracegc\"",
        computed_lexeme: None,
        line: 5,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 7,
    },
    Token {
        kind: String,
        lexeme: "\"testing stack overflow detection\"",
        computed_lexeme: None,
        line: 7,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 15,
    },
    Token {
        kind: Function,
        lexeme: "function",
        computed_lexeme: None,
        line: 15,
    },
    Token {
        kind: Identifier,
        lexeme: "checkerror",
        computed_lexeme: None,
        line: 15,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 15,
    },
    Token {
        kind: Identifier,
        lexeme: "msg",
        computed_lexeme: None,
        line: 15,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 15,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 15,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 15,
    },
    Token {
        kind: TripleDot,
        lexeme: "...",
        computed_lexeme: None,
        line: 15,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 15,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 16,
    },
    Token {
        kind: Identifier,
        lexeme: "s",
        computed_lexeme: None,
        line: 16,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 16,
    },
    Token {
        kind: Identifier,
        lexeme: "err",
        computed_lexeme: None,
        line: 16,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 16,
    },
    Token {
        kind: Identifier,
        lexeme: "pcall",
        computed_lexeme: None,
        line: 16,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 16,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 16,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 16,
    },
    Token {
        kind: TripleDot,
        lexeme: "...",
        computed_lexeme: None,
        line: 16,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 16,
    },
    Token {
        kind: Identifier,
        lexeme: "assert",
        computed_lexeme: None,
        line: 17,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 17,
    },
    Token {
        kind: Not,
        lexeme: "not",
        computed_lexeme: None,
        line: 17,
    },
    Token {
        kind: Identifier,
        lexeme: "s",
        computed_lexeme: None,
        line: 17,
    },
    Token {
        kind: And,
        lexeme: "and",
        computed_lexeme: None,
        line: 17,
    },
    Token {
        kind: Identifier,
        lexeme: "string",
        computed_lexeme: None,
        line: 17,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 17,
    },
    Token {
        kind: Identifier,
        lexeme: "find",
        computed_lexeme: None,
        line: 17,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 17,
    },
    Token {
        kind: Identifier,
        lexeme: "err",
        computed_lexeme: None,
        line: 17,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 17,
    },
    Token {
        kind: Identifier,
        lexeme: "msg",
        computed_lexeme: None,
        line: 17,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 17,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 17,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 18,
    },
    Token {
        kind: Do,
        lexeme: "do",
        computed_lexeme: None,
        line: 20,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 20,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 20,
    },
    Token {
        kind: String,
        lexeme: "\"testing stack overflow in message handling\"",
        computed_lexeme: None,
        line: 20,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 20,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 21,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 21,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 21,
    },
    Token {
        kind: Number,
        lexeme: "0",
        computed_lexeme: Some(
            "0",
        ),
        line: 21,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 22,
    },
    Token {
        kind: Function,
        lexeme: "function",
        computed_lexeme: None,
        line: 22,
    },
    Token {
        kind: Identifier,
        lexeme: "loop",
        computed_lexeme: None,
        line: 22,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 22,
    },
    Token {
        kind: Identifier,
        lexeme: "x",
        computed_lexeme: None,
        line: 22,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 22,
    },
    Token {
        kind: Identifier,
        lexeme: "y",
        computed_lexeme: None,
        line: 22,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 22,
    },
    Token {
        kind: Identifier,
        lexeme: "z",
        computed_lexeme: None,
        line: 22,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 22,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 23,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 23,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 23,
    },
    Token {
        kind: Plus,
        lexeme: "+",
        computed_lexeme: None,
        line: 23,
    },
    Token {
        kind: Number,
        lexeme: "1",
        computed_lexeme: Some(
            "1",
        ),
        line: 23,
    },
    Token {
        kind: Return,
        lexeme: "return",
        computed_lexeme: None,
        line: 24,
    },
    Token {
        kind: Number,
        lexeme: "1",
        computed_lexeme: Some(
            "1",
        ),
        line: 24,
    },
    Token {
        kind: Plus,
        lexeme: "+",
        computed_lexeme: None,
        line: 24,
    },
    Token {
        kind: Identifier,
        lexeme: "loop",
        computed_lexeme: None,
        line: 24,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 24,
    },
    Token {
        kind: Identifier,
        lexeme: "x",
        computed_lexeme: None,
        line: 24,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 24,
    },
    Token {
        kind: Identifier,
        lexeme: "y",
        computed_lexeme: None,
        line: 24,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 24,
    },
    Token {
        kind: Identifier,
        lexeme: "z",
        computed_lexeme: None,
        line: 24,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 24,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 25,
    },
    Token {
        kind: Identifier,
        lexeme: "tracegc",
        computed_lexeme: None,
        line: 26,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 26,
    },
    Token {
        kind: Identifier,
        lexeme: "stop",
        computed_lexeme: None,
        line: 26,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 26,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 26,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 27,
    },
    Token {
        kind: Identifier,
        lexeme: "res",
        computed_lexeme: None,
        line: 27,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 27,
    },
    Token {
        kind: Identifier,
        lexeme: "msg",
        computed_lexeme: None,
        line: 27,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 27,
    },
    Token {
        kind: Identifier,
        lexeme: "xpcall",
        computed_lexeme: None,
        line: 27,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 27,
    },
    Token {
        kind: Identifier,
        lexeme: "loop",
        computed_lexeme: None,
        line: 27,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 27,
    },
    Token {
        kind: Identifier,
        lexeme: "loop",
        computed_lexeme: None,
        line: 27,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 27,
    },
    Token {
        kind: Identifier,
        lexeme: "tracegc",
        computed_lexeme: None,
        line: 28,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 28,
    },
    Token {
        kind: Identifier,
        lexeme: "start",
        computed_lexeme: None,
        line: 28,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 28,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 28,
    },
    Token {
        kind: Identifier,
        lexeme: "assert",
        computed_lexeme: None,
        line: 29,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 29,
    },
    Token {
        kind: Identifier,
        lexeme: "msg",
        computed_lexeme: None,
        line: 29,
    },
    Token {
        kind: DoubleEquals,
        lexeme: "==",
        computed_lexeme: None,
        line: 29,
    },
    Token {
        kind: String,
        lexeme: "\"error in error handling\"",
        computed_lexeme: None,
        line: 29,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 29,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 30,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 30,
    },
    Token {
        kind: String,
        lexeme: "\"final count: \"",
        computed_lexeme: None,
        line: 30,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 30,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 30,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 30,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 31,
    },
    Token {
        kind: Do,
        lexeme: "do",
        computed_lexeme: None,
        line: 35,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 35,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 35,
    },
    Token {
        kind: String,
        lexeme: "\"testing recursion inside pattern matching\"",
        computed_lexeme: None,
        line: 35,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 35,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 36,
    },
    Token {
        kind: Function,
        lexeme: "function",
        computed_lexeme: None,
        line: 36,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 36,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 36,
    },
    Token {
        kind: Identifier,
        lexeme: "size",
        computed_lexeme: None,
        line: 36,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 36,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 37,
    },
    Token {
        kind: Identifier,
        lexeme: "s",
        computed_lexeme: None,
        line: 37,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 37,
    },
    Token {
        kind: Identifier,
        lexeme: "string",
        computed_lexeme: None,
        line: 37,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 37,
    },
    Token {
        kind: Identifier,
        lexeme: "rep",
        computed_lexeme: None,
        line: 37,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 37,
    },
    Token {
        kind: String,
        lexeme: "\"a\"",
        computed_lexeme: None,
        line: 37,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 37,
    },
    Token {
        kind: Identifier,
        lexeme: "size",
        computed_lexeme: None,
        line: 37,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 37,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 38,
    },
    Token {
        kind: Identifier,
        lexeme: "p",
        computed_lexeme: None,
        line: 38,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 38,
    },
    Token {
        kind: Identifier,
        lexeme: "string",
        computed_lexeme: None,
        line: 38,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 38,
    },
    Token {
        kind: Identifier,
        lexeme: "rep",
        computed_lexeme: None,
        line: 38,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 38,
    },
    Token {
        kind: String,
        lexeme: "\".?\"",
        computed_lexeme: None,
        line: 38,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 38,
    },
    Token {
        kind: Identifier,
        lexeme: "size",
        computed_lexeme: None,
        line: 38,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 38,
    },
    Token {
        kind: Return,
        lexeme: "return",
        computed_lexeme: None,
        line: 39,
    },
    Token {
        kind: Identifier,
        lexeme: "string",
        computed_lexeme: None,
        line: 39,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 39,
    },
    Token {
        kind: Identifier,
        lexeme: "match",
        computed_lexeme: None,
        line: 39,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 39,
    },
    Token {
        kind: Identifier,
        lexeme: "s",
        computed_lexeme: None,
        line: 39,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 39,
    },
    Token {
        kind: Identifier,
        lexeme: "p",
        computed_lexeme: None,
        line: 39,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 39,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 40,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 41,
    },
    Token {
        kind: Identifier,
        lexeme: "m",
        computed_lexeme: None,
        line: 41,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 41,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 41,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 41,
    },
    Token {
        kind: Number,
        lexeme: "80",
        computed_lexeme: Some(
            "80",
        ),
        line: 41,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 41,
    },
    Token {
        kind: Identifier,
        lexeme: "assert",
        computed_lexeme: None,
        line: 42,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 42,
    },
    Token {
        kind: Hash,
        lexeme: "#",
        computed_lexeme: None,
        line: 42,
    },
    Token {
        kind: Identifier,
        lexeme: "m",
        computed_lexeme: None,
        line: 42,
    },
    Token {
        kind: DoubleEquals,
        lexeme: "==",
        computed_lexeme: None,
        line: 42,
    },
    Token {
        kind: Number,
        lexeme: "80",
        computed_lexeme: Some(
            "80",
        ),
        line: 42,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 42,
    },
    Token {
        kind: Identifier,
        lexeme: "checkerror",
        computed_lexeme: None,
        line: 43,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 43,
    },
    Token {
        kind: String,
        lexeme: "\"too complex\"",
        computed_lexeme: None,
        line: 43,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 43,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 43,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 43,
    },
    Token {
        kind: Number,
        lexeme: "2000",
        computed_lexeme: Some(
            "2000",
        ),
        line: 43,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 43,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 44,
    },
    Token {
        kind: Do,
        lexeme: "do",
        computed_lexeme: None,
        line: 47,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 47,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 47,
    },
    Token {
        kind: String,
        lexeme: "\"testing stack-overflow in recursive 'gsub'\"",
        computed_lexeme: None,
        line: 47,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 47,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 48,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 48,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 48,
    },
    Token {
        kind: Number,
        lexeme: "0",
        computed_lexeme: Some(
            "0",
        ),
        line: 48,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 49,
    },
    Token {
        kind: Function,
        lexeme: "function",
        computed_lexeme: None,
        line: 49,
    },
    Token {
        kind: Identifier,
        lexeme: "foo",
        computed_lexeme: None,
        line: 49,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 49,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 49,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 50,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 50,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 50,
    },
    Token {
        kind: Plus,
        lexeme: "+",
        computed_lexeme: None,
        line: 50,
    },
    Token {
        kind: Number,
        lexeme: "1",
        computed_lexeme: Some(
            "1",
        ),
        line: 50,
    },
    Token {
        kind: Identifier,
        lexeme: "string",
        computed_lexeme: None,
        line: 51,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 51,
    },
    Token {
        kind: Identifier,
        lexeme: "gsub",
        computed_lexeme: None,
        line: 51,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 51,
    },
    Token {
        kind: String,
        lexeme: "\"a\"",
        computed_lexeme: None,
        line: 51,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 51,
    },
    Token {
        kind: String,
        lexeme: "\".\"",
        computed_lexeme: None,
        line: 51,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 51,
    },
    Token {
        kind: Identifier,
        lexeme: "foo",
        computed_lexeme: None,
        line: 51,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 51,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 52,
    },
    Token {
        kind: Identifier,
        lexeme: "checkerror",
        computed_lexeme: None,
        line: 53,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 53,
    },
    Token {
        kind: String,
        lexeme: "\"stack overflow\"",
        computed_lexeme: None,
        line: 53,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 53,
    },
    Token {
        kind: Identifier,
        lexeme: "foo",
        computed_lexeme: None,
        line: 53,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 53,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 54,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 54,
    },
    Token {
        kind: String,
        lexeme: "\"final count: \"",
        computed_lexeme: None,
        line: 54,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 54,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 54,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 54,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 56,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 56,
    },
    Token {
        kind: String,
        lexeme: "\"testing stack-overflow in recursive 'gsub' with metatables\"",
        computed_lexeme: None,
        line: 56,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 56,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 57,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 57,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 57,
    },
    Token {
        kind: Number,
        lexeme: "0",
        computed_lexeme: Some(
            "0",
        ),
        line: 57,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 58,
    },
    Token {
        kind: Identifier,
        lexeme: "t",
        computed_lexeme: None,
        line: 58,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 58,
    },
    Token {
        kind: Identifier,
        lexeme: "setmetatable",
        computed_lexeme: None,
        line: 58,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 58,
    },
    Token {
        kind: LeftBrace,
        lexeme: "{",
        computed_lexeme: None,
        line: 58,
    },
    Token {
        kind: RightBrace,
        lexeme: "}",
        computed_lexeme: None,
        line: 58,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 58,
    },
    Token {
        kind: LeftBrace,
        lexeme: "{",
        computed_lexeme: None,
        line: 58,
    },
    Token {
        kind: Identifier,
        lexeme: "__index",
        computed_lexeme: None,
        line: 58,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 58,
    },
    Token {
        kind: Identifier,
        lexeme: "foo",
        computed_lexeme: None,
        line: 58,
    },
    Token {
        kind: RightBrace,
        lexeme: "}",
        computed_lexeme: None,
        line: 58,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 58,
    },
    Token {
        kind: Identifier,
        lexeme: "foo",
        computed_lexeme: None,
        line: 59,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 59,
    },
    Token {
        kind: Function,
        lexeme: "function",
        computed_lexeme: None,
        line: 59,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 59,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 59,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 60,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 60,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 60,
    },
    Token {
        kind: Plus,
        lexeme: "+",
        computed_lexeme: None,
        line: 60,
    },
    Token {
        kind: Number,
        lexeme: "1",
        computed_lexeme: Some(
            "1",
        ),
        line: 60,
    },
    Token {
        kind: Identifier,
        lexeme: "string",
        computed_lexeme: None,
        line: 61,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 61,
    },
    Token {
        kind: Identifier,
        lexeme: "gsub",
        computed_lexeme: None,
        line: 61,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 61,
    },
    Token {
        kind: String,
        lexeme: "\"a\"",
        computed_lexeme: None,
        line: 61,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 61,
    },
    Token {
        kind: String,
        lexeme: "\".\"",
        computed_lexeme: None,
        line: 61,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 61,
    },
    Token {
        kind: Identifier,
        lexeme: "t",
        computed_lexeme: None,
        line: 61,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 61,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 62,
    },
    Token {
        kind: Identifier,
        lexeme: "checkerror",
        computed_lexeme: None,
        line: 63,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 63,
    },
    Token {
        kind: String,
        lexeme: "\"stack overflow\"",
        computed_lexeme: None,
        line: 63,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 63,
    },
    Token {
        kind: Identifier,
        lexeme: "foo",
        computed_lexeme: None,
        line: 63,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 63,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 64,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 64,
    },
    Token {
        kind: String,
        lexeme: "\"final count: \"",
        computed_lexeme: None,
        line: 64,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 64,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 64,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 64,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 65,
    },
    Token {
        kind: Do,
        lexeme: "do",
        computed_lexeme: None,
        line: 68,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 69,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 69,
    },
    Token {
        kind: String,
        lexeme: "\"testing limits in coroutines inside deep calls\"",
        computed_lexeme: None,
        line: 69,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 69,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 70,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 70,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 70,
    },
    Token {
        kind: Number,
        lexeme: "0",
        computed_lexeme: Some(
            "0",
        ),
        line: 70,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 71,
    },
    Token {
        kind: Identifier,
        lexeme: "lim",
        computed_lexeme: None,
        line: 71,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 71,
    },
    Token {
        kind: Number,
        lexeme: "1000",
        computed_lexeme: Some(
            "1000",
        ),
        line: 71,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 72,
    },
    Token {
        kind: Function,
        lexeme: "function",
        computed_lexeme: None,
        line: 72,
    },
    Token {
        kind: Identifier,
        lexeme: "stack",
        computed_lexeme: None,
        line: 72,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 72,
    },
    Token {
        kind: Identifier,
        lexeme: "n",
        computed_lexeme: None,
        line: 72,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 72,
    },
    Token {
        kind: If,
        lexeme: "if",
        computed_lexeme: None,
        line: 73,
    },
    Token {
        kind: Identifier,
        lexeme: "n",
        computed_lexeme: None,
        line: 73,
    },
    Token {
        kind: GreaterThan,
        lexeme: ">",
        computed_lexeme: None,
        line: 73,
    },
    Token {
        kind: Number,
        lexeme: "0",
        computed_lexeme: Some(
            "0",
        ),
        line: 73,
    },
    Token {
        kind: Then,
        lexeme: "then",
        computed_lexeme: None,
        line: 73,
    },
    Token {
        kind: Return,
        lexeme: "return",
        computed_lexeme: None,
        line: 73,
    },
    Token {
        kind: Identifier,
        lexeme: "stack",
        computed_lexeme: None,
        line: 73,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 73,
    },
    Token {
        kind: Identifier,
        lexeme: "n",
        computed_lexeme: None,
        line: 73,
    },
    Token {
        kind: Minus,
        lexeme: "-",
        computed_lexeme: None,
        line: 73,
    },
    Token {
        kind: Number,
        lexeme: "1",
        computed_lexeme: Some(
            "1",
        ),
        line: 73,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 73,
    },
    Token {
        kind: Plus,
        lexeme: "+",
        computed_lexeme: None,
        line: 73,
    },
    Token {
        kind: Number,
        lexeme: "1",
        computed_lexeme: Some(
            "1",
        ),
        line: 73,
    },
    Token {
        kind: Else,
        lexeme: "else",
        computed_lexeme: None,
        line: 74,
    },
    Token {
        kind: Identifier,
        lexeme: "coroutine",
        computed_lexeme: None,
        line: 74,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 74,
    },
    Token {
        kind: Identifier,
        lexeme: "wrap",
        computed_lexeme: None,
        line: 74,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 74,
    },
    Token {
        kind: Function,
        lexeme: "function",
        computed_lexeme: None,
        line: 74,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 74,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 74,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 75,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 75,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 75,
    },
    Token {
        kind: Plus,
        lexeme: "+",
        computed_lexeme: None,
        line: 75,
    },
    Token {
        kind: Number,
        lexeme: "1",
        computed_lexeme: Some(
            "1",
        ),
        line: 75,
    },
    Token {
        kind: Identifier,
        lexeme: "stack",
        computed_lexeme: None,
        line: 76,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 76,
    },
    Token {
        kind: Identifier,
        lexeme: "lim",
        computed_lexeme: None,
        line: 76,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 76,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 77,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 77,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 77,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 77,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 78,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 79,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: Identifier,
        lexeme: "st",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: Identifier,
        lexeme: "msg",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: Identifier,
        lexeme: "xpcall",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: Identifier,
        lexeme: "stack",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: Function,
        lexeme: "function",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: Return,
        lexeme: "return",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: String,
        lexeme: "\"ok\"",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: Identifier,
        lexeme: "lim",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 81,
    },
    Token {
        kind: Identifier,
        lexeme: "assert",
        computed_lexeme: None,
        line: 82,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 82,
    },
    Token {
        kind: Not,
        lexeme: "not",
        computed_lexeme: None,
        line: 82,
    },
    Token {
        kind: Identifier,
        lexeme: "st",
        computed_lexeme: None,
        line: 82,
    },
    Token {
        kind: And,
        lexeme: "and",
        computed_lexeme: None,
        line: 82,
    },
    Token {
        kind: Identifier,
        lexeme: "msg",
        computed_lexeme: None,
        line: 82,
    },
    Token {
        kind: DoubleEquals,
        lexeme: "==",
        computed_lexeme: None,
        line: 82,
    },
    Token {
        kind: String,
        lexeme: "\"ok\"",
        computed_lexeme: None,
        line: 82,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 82,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 83,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 83,
    },
    Token {
        kind: String,
        lexeme: "\"final count: \"",
        computed_lexeme: None,
        line: 83,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 83,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 83,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 83,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 84,
    },
    Token {
        kind: Do,
        lexeme: "do",
        computed_lexeme: None,
        line: 87,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 88,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 88,
    },
    Token {
        kind: String,
        lexeme: "\"nesting of resuming yielded coroutines\"",
        computed_lexeme: None,
        line: 88,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 88,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 89,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 89,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 89,
    },
    Token {
        kind: Number,
        lexeme: "0",
        computed_lexeme: Some(
            "0",
        ),
        line: 89,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 91,
    },
    Token {
        kind: Function,
        lexeme: "function",
        computed_lexeme: None,
        line: 91,
    },
    Token {
        kind: Identifier,
        lexeme: "body",
        computed_lexeme: None,
        line: 91,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 91,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 91,
    },
    Token {
        kind: Identifier,
        lexeme: "coroutine",
        computed_lexeme: None,
        line: 92,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 92,
    },
    Token {
        kind: Identifier,
        lexeme: "yield",
        computed_lexeme: None,
        line: 92,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 92,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 92,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 93,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 93,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 93,
    },
    Token {
        kind: Identifier,
        lexeme: "coroutine",
        computed_lexeme: None,
        line: 93,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 93,
    },
    Token {
        kind: Identifier,
        lexeme: "wrap",
        computed_lexeme: None,
        line: 93,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 93,
    },
    Token {
        kind: Identifier,
        lexeme: "body",
        computed_lexeme: None,
        line: 93,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 93,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 94,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 94,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 94,
    },
    Token {
        kind: Semicolon,
        lexeme: ";",
        computed_lexeme: None,
        line: 94,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 95,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 95,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 95,
    },
    Token {
        kind: Plus,
        lexeme: "+",
        computed_lexeme: None,
        line: 95,
    },
    Token {
        kind: Number,
        lexeme: "1",
        computed_lexeme: Some(
            "1",
        ),
        line: 95,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 96,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 96,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 96,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 97,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 99,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 99,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 99,
    },
    Token {
        kind: Identifier,
        lexeme: "coroutine",
        computed_lexeme: None,
        line: 99,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 99,
    },
    Token {
        kind: Identifier,
        lexeme: "wrap",
        computed_lexeme: None,
        line: 99,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 99,
    },
    Token {
        kind: Identifier,
        lexeme: "body",
        computed_lexeme: None,
        line: 99,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 99,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 100,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 100,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 100,
    },
    Token {
        kind: Identifier,
        lexeme: "assert",
        computed_lexeme: None,
        line: 101,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 101,
    },
    Token {
        kind: Not,
        lexeme: "not",
        computed_lexeme: None,
        line: 101,
    },
    Token {
        kind: Identifier,
        lexeme: "pcall",
        computed_lexeme: None,
        line: 101,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 101,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 101,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 101,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 101,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 102,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 102,
    },
    Token {
        kind: String,
        lexeme: "\"final count: \"",
        computed_lexeme: None,
        line: 102,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 102,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 102,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 102,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 103,
    },
    Token {
        kind: Do,
        lexeme: "do",
        computed_lexeme: None,
        line: 106,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 107,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 107,
    },
    Token {
        kind: String,
        lexeme: "\"nesting coroutines running after recoverable errors\"",
        computed_lexeme: None,
        line: 107,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 107,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 108,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 108,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 108,
    },
    Token {
        kind: Number,
        lexeme: "0",
        computed_lexeme: Some(
            "0",
        ),
        line: 108,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 109,
    },
    Token {
        kind: Function,
        lexeme: "function",
        computed_lexeme: None,
        line: 109,
    },
    Token {
        kind: Identifier,
        lexeme: "foo",
        computed_lexeme: None,
        line: 109,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 109,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 109,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 110,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 110,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 110,
    },
    Token {
        kind: Plus,
        lexeme: "+",
        computed_lexeme: None,
        line: 110,
    },
    Token {
        kind: Number,
        lexeme: "1",
        computed_lexeme: Some(
            "1",
        ),
        line: 110,
    },
    Token {
        kind: Identifier,
        lexeme: "pcall",
        computed_lexeme: None,
        line: 111,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 111,
    },
    Token {
        kind: Number,
        lexeme: "1",
        computed_lexeme: Some(
            "1",
        ),
        line: 111,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 111,
    },
    Token {
        kind: Identifier,
        lexeme: "coroutine",
        computed_lexeme: None,
        line: 113,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 113,
    },
    Token {
        kind: Identifier,
        lexeme: "wrap",
        computed_lexeme: None,
        line: 113,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 113,
    },
    Token {
        kind: Identifier,
        lexeme: "foo",
        computed_lexeme: None,
        line: 113,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 113,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 113,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 113,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 114,
    },
    Token {
        kind: Identifier,
        lexeme: "checkerror",
        computed_lexeme: None,
        line: 115,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 115,
    },
    Token {
        kind: String,
        lexeme: "\"C stack overflow\"",
        computed_lexeme: None,
        line: 115,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 115,
    },
    Token {
        kind: Identifier,
        lexeme: "foo",
        computed_lexeme: None,
        line: 115,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 115,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 116,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 116,
    },
    Token {
        kind: String,
        lexeme: "\"final count: \"",
        computed_lexeme: None,
        line: 116,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 116,
    },
    Token {
        kind: Identifier,
        lexeme: "count",
        computed_lexeme: None,
        line: 116,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 116,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 117,
    },
    Token {
        kind: If,
        lexeme: "if",
        computed_lexeme: None,
        line: 120,
    },
    Token {
        kind: Identifier,
        lexeme: "T",
        computed_lexeme: None,
        line: 120,
    },
    Token {
        kind: Then,
        lexeme: "then",
        computed_lexeme: None,
        line: 120,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 121,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 121,
    },
    Token {
        kind: String,
        lexeme: "\"testing stack recovery\"",
        computed_lexeme: None,
        line: 121,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 121,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 122,
    },
    Token {
        kind: Identifier,
        lexeme: "N",
        computed_lexeme: None,
        line: 122,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 122,
    },
    Token {
        kind: Number,
        lexeme: "0",
        computed_lexeme: Some(
            "0",
        ),
        line: 122,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 123,
    },
    Token {
        kind: Identifier,
        lexeme: "LIM",
        computed_lexeme: None,
        line: 123,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 123,
    },
    Token {
        kind: Minus,
        lexeme: "-",
        computed_lexeme: None,
        line: 123,
    },
    Token {
        kind: Number,
        lexeme: "1",
        computed_lexeme: Some(
            "1",
        ),
        line: 123,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 127,
    },
    Token {
        kind: Identifier,
        lexeme: "stack1",
        computed_lexeme: None,
        line: 127,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 128,
    },
    Token {
        kind: Identifier,
        lexeme: "dummy",
        computed_lexeme: None,
        line: 128,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 130,
    },
    Token {
        kind: Function,
        lexeme: "function",
        computed_lexeme: None,
        line: 130,
    },
    Token {
        kind: Identifier,
        lexeme: "err",
        computed_lexeme: None,
        line: 130,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 130,
    },
    Token {
        kind: Identifier,
        lexeme: "msg",
        computed_lexeme: None,
        line: 130,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 130,
    },
    Token {
        kind: Identifier,
        lexeme: "assert",
        computed_lexeme: None,
        line: 131,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 131,
    },
    Token {
        kind: Identifier,
        lexeme: "string",
        computed_lexeme: None,
        line: 131,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 131,
    },
    Token {
        kind: Identifier,
        lexeme: "find",
        computed_lexeme: None,
        line: 131,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 131,
    },
    Token {
        kind: Identifier,
        lexeme: "msg",
        computed_lexeme: None,
        line: 131,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 131,
    },
    Token {
        kind: String,
        lexeme: "\"stack overflow\"",
        computed_lexeme: None,
        line: 131,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 131,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 131,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 132,
    },
    Token {
        kind: Identifier,
        lexeme: "_",
        computed_lexeme: None,
        line: 132,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 132,
    },
    Token {
        kind: Identifier,
        lexeme: "stacknow",
        computed_lexeme: None,
        line: 132,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 132,
    },
    Token {
        kind: Identifier,
        lexeme: "T",
        computed_lexeme: None,
        line: 132,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 132,
    },
    Token {
        kind: Identifier,
        lexeme: "stacklevel",
        computed_lexeme: None,
        line: 132,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 132,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 132,
    },
    Token {
        kind: Identifier,
        lexeme: "assert",
        computed_lexeme: None,
        line: 133,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 133,
    },
    Token {
        kind: Identifier,
        lexeme: "stacknow",
        computed_lexeme: None,
        line: 133,
    },
    Token {
        kind: DoubleEquals,
        lexeme: "==",
        computed_lexeme: None,
        line: 133,
    },
    Token {
        kind: Identifier,
        lexeme: "stack1",
        computed_lexeme: None,
        line: 133,
    },
    Token {
        kind: Plus,
        lexeme: "+",
        computed_lexeme: None,
        line: 133,
    },
    Token {
        kind: Number,
        lexeme: "200",
        computed_lexeme: Some(
            "200",
        ),
        line: 133,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 133,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 134,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 141,
    },
    Token {
        kind: Function,
        lexeme: "function",
        computed_lexeme: None,
        line: 141,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 141,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 141,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 141,
    },
    Token {
        kind: Identifier,
        lexeme: "dummy",
        computed_lexeme: None,
        line: 142,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 142,
    },
    Token {
        kind: Identifier,
        lexeme: "stack1",
        computed_lexeme: None,
        line: 142,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 142,
    },
    Token {
        kind: Identifier,
        lexeme: "T",
        computed_lexeme: None,
        line: 142,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 142,
    },
    Token {
        kind: Identifier,
        lexeme: "stacklevel",
        computed_lexeme: None,
        line: 142,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 142,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 142,
    },
    Token {
        kind: If,
        lexeme: "if",
        computed_lexeme: None,
        line: 143,
    },
    Token {
        kind: Identifier,
        lexeme: "N",
        computed_lexeme: None,
        line: 143,
    },
    Token {
        kind: DoubleEquals,
        lexeme: "==",
        computed_lexeme: None,
        line: 143,
    },
    Token {
        kind: Identifier,
        lexeme: "LIM",
        computed_lexeme: None,
        line: 143,
    },
    Token {
        kind: Then,
        lexeme: "then",
        computed_lexeme: None,
        line: 143,
    },
    Token {
        kind: Identifier,
        lexeme: "xpcall",
        computed_lexeme: None,
        line: 144,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 144,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 144,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 144,
    },
    Token {
        kind: Identifier,
        lexeme: "err",
        computed_lexeme: None,
        line: 144,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 144,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 145,
    },
    Token {
        kind: Identifier,
        lexeme: "_",
        computed_lexeme: None,
        line: 145,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 145,
    },
    Token {
        kind: Identifier,
        lexeme: "stacknow",
        computed_lexeme: None,
        line: 145,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 145,
    },
    Token {
        kind: Identifier,
        lexeme: "T",
        computed_lexeme: None,
        line: 145,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 145,
    },
    Token {
        kind: Identifier,
        lexeme: "stacklevel",
        computed_lexeme: None,
        line: 145,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 145,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 145,
    },
    Token {
        kind: Identifier,
        lexeme: "assert",
        computed_lexeme: None,
        line: 146,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 146,
    },
    Token {
        kind: Identifier,
        lexeme: "stacknow",
        computed_lexeme: None,
        line: 146,
    },
    Token {
        kind: DoubleEquals,
        lexeme: "==",
        computed_lexeme: None,
        line: 146,
    },
    Token {
        kind: Identifier,
        lexeme: "stack1",
        computed_lexeme: None,
        line: 146,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 146,
    },
    Token {
        kind: Return,
        lexeme: "return",
        computed_lexeme: None,
        line: 147,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 148,
    },
    Token {
        kind: Identifier,
        lexeme: "N",
        computed_lexeme: None,
        line: 149,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 149,
    },
    Token {
        kind: Identifier,
        lexeme: "N",
        computed_lexeme: None,
        line: 149,
    },
    Token {
        kind: Plus,
        lexeme: "+",
        computed_lexeme: None,
        line: 149,
    },
    Token {
        kind: Number,
        lexeme: "1",
        computed_lexeme: Some(
            "1",
        ),
        line: 149,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 150,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 150,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 150,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 151,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 153,
    },
    Token {
        kind: Identifier,
        lexeme: "topB",
        computed_lexeme: None,
        line: 153,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 153,
    },
    Token {
        kind: Identifier,
        lexeme: "sizeB",
        computed_lexeme: None,
        line: 153,
    },
    Token {
        kind: Local,
        lexeme: "local",
        computed_lexeme: None,
        line: 154,
    },
    Token {
        kind: Identifier,
        lexeme: "topA",
        computed_lexeme: None,
        line: 154,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 154,
    },
    Token {
        kind: Identifier,
        lexeme: "sizeA",
        computed_lexeme: None,
        line: 154,
    },
    Token {
        kind: Identifier,
        lexeme: "topB",
        computed_lexeme: None,
        line: 155,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 155,
    },
    Token {
        kind: Identifier,
        lexeme: "sizeB",
        computed_lexeme: None,
        line: 155,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 155,
    },
    Token {
        kind: Identifier,
        lexeme: "T",
        computed_lexeme: None,
        line: 155,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 155,
    },
    Token {
        kind: Identifier,
        lexeme: "stacklevel",
        computed_lexeme: None,
        line: 155,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 155,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 155,
    },
    Token {
        kind: Identifier,
        lexeme: "tracegc",
        computed_lexeme: None,
        line: 156,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 156,
    },
    Token {
        kind: Identifier,
        lexeme: "stop",
        computed_lexeme: None,
        line: 156,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 156,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 156,
    },
    Token {
        kind: Identifier,
        lexeme: "xpcall",
        computed_lexeme: None,
        line: 157,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 157,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 157,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 157,
    },
    Token {
        kind: Identifier,
        lexeme: "err",
        computed_lexeme: None,
        line: 157,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 157,
    },
    Token {
        kind: Identifier,
        lexeme: "tracegc",
        computed_lexeme: None,
        line: 158,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 158,
    },
    Token {
        kind: Identifier,
        lexeme: "start",
        computed_lexeme: None,
        line: 158,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 158,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 158,
    },
    Token {
        kind: Identifier,
        lexeme: "topA",
        computed_lexeme: None,
        line: 159,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 159,
    },
    Token {
        kind: Identifier,
        lexeme: "sizeA",
        computed_lexeme: None,
        line: 159,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 159,
    },
    Token {
        kind: Identifier,
        lexeme: "T",
        computed_lexeme: None,
        line: 159,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 159,
    },
    Token {
        kind: Identifier,
        lexeme: "stacklevel",
        computed_lexeme: None,
        line: 159,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 159,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 159,
    },
    Token {
        kind: Identifier,
        lexeme: "assert",
        computed_lexeme: None,
        line: 161,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 161,
    },
    Token {
        kind: Identifier,
        lexeme: "topA",
        computed_lexeme: None,
        line: 161,
    },
    Token {
        kind: DoubleEquals,
        lexeme: "==",
        computed_lexeme: None,
        line: 161,
    },
    Token {
        kind: Identifier,
        lexeme: "topB",
        computed_lexeme: None,
        line: 161,
    },
    Token {
        kind: And,
        lexeme: "and",
        computed_lexeme: None,
        line: 161,
    },
    Token {
        kind: Identifier,
        lexeme: "sizeA",
        computed_lexeme: None,
        line: 161,
    },
    Token {
        kind: LessThan,
        lexeme: "<",
        computed_lexeme: None,
        line: 161,
    },
    Token {
        kind: Identifier,
        lexeme: "sizeB",
        computed_lexeme: None,
        line: 161,
    },
    Token {
        kind: Star,
        lexeme: "*",
        computed_lexeme: None,
        line: 161,
    },
    Token {
        kind: Number,
        lexeme: "2",
        computed_lexeme: Some(
            "2",
        ),
        line: 161,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 161,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 162,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 162,
    },
    Token {
        kind: Identifier,
        lexeme: "string",
        computed_lexeme: None,
        line: 162,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 162,
    },
    Token {
        kind: Identifier,
        lexeme: "format",
        computed_lexeme: None,
        line: 162,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 162,
    },
    Token {
        kind: String,
        lexeme: "\"maximum stack size: %d\"",
        computed_lexeme: None,
        line: 162,
    },
    Token {
        kind: Comma,
        lexeme: ",",
        computed_lexeme: None,
        line: 162,
    },
    Token {
        kind: Identifier,
        lexeme: "stack1",
        computed_lexeme: None,
        line: 162,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 162,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 162,
    },
    Token {
        kind: Identifier,
        lexeme: "LIM",
        computed_lexeme: None,
        line: 163,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 163,
    },
    Token {
        kind: Identifier,
        lexeme: "N",
        computed_lexeme: None,
        line: 163,
    },
    Token {
        kind: Identifier,
        lexeme: "N",
        computed_lexeme: None,
        line: 164,
    },
    Token {
        kind: Equals,
        lexeme: "=",
        computed_lexeme: None,
        line: 164,
    },
    Token {
        kind: Number,
        lexeme: "0",
        computed_lexeme: Some(
            "0",
        ),
        line: 164,
    },
    Token {
        kind: Identifier,
        lexeme: "tracegc",
        computed_lexeme: None,
        line: 165,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 165,
    },
    Token {
        kind: Identifier,
        lexeme: "stop",
        computed_lexeme: None,
        line: 165,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 165,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 165,
    },
    Token {
        kind: Identifier,
        lexeme: "f",
        computed_lexeme: None,
        line: 166,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 166,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 166,
    },
    Token {
        kind: Identifier,
        lexeme: "tracegc",
        computed_lexeme: None,
        line: 167,
    },
    Token {
        kind: Dot,
        lexeme: ".",
        computed_lexeme: None,
        line: 167,
    },
    Token {
        kind: Identifier,
        lexeme: "start",
        computed_lexeme: None,
        line: 167,
    },
    Token {
        kind: LeftParen,
        lexeme: "(",
        computed_lexeme: None,
        line: 167,
    },
    Token {
        kind: RightParen,
        lexeme: ")",
        computed_lexeme: None,
        line: 167,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 168,
    },
    Token {
        kind: String,
        lexeme: "\"+\"",
        computed_lexeme: None,
        line: 168,
    },
    Token {
        kind: End,
        lexeme: "end",
        computed_lexeme: None,
        line: 169,
    },
    Token {
        kind: Identifier,
        lexeme: "print",
        computed_lexeme: None,
        line: 171,
    },
    Token {
        kind: String,
        lexeme: "'OK'",
        computed_lexeme: None,
        line: 171,
    },
]
